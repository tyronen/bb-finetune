{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14c9f0ac",
   "metadata": {},
   "source": [
    "### Sort mem\n",
    "Jax shotguns a bunch of memoery on the GPU but it doesn't need it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5820d38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\n",
    "os.environ['XLA_PYTHON_CLIENT_ALLOCATOR'] = 'platform'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5f60a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX device: cuda:0\n",
      "JAX memory stats not available\n",
      "\n",
      "NVIDIA GPU memory:\n",
      "  Used: 0.11 GB\n",
      "  Total: 8.00 GB\n",
      "  Utilization: 1.3%\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import subprocess\n",
    "\n",
    "def check_memory_usage():\n",
    "    # JAX device info\n",
    "    device = jax.devices()[0]\n",
    "    print(f\"JAX device: {device}\")\n",
    "    \n",
    "    # Try JAX memory stats (newer versions)\n",
    "    if hasattr(device, 'memory_stats'):\n",
    "        stats = device.memory_stats()\n",
    "        if stats is not None:\n",
    "            print(f\"JAX memory stats:\")\n",
    "            print(f\"  Bytes in use: {stats.get('bytes_in_use', 0) / 1024**3:.2f} GB\")\n",
    "            print(f\"  Pool bytes: {stats.get('pool_bytes', 0) / 1024**3:.2f} GB\")\n",
    "            print(f\"  Peak bytes: {stats.get('peak_bytes_in_use', 0) / 1024**3:.2f} GB\")\n",
    "        else:\n",
    "            print(\"JAX memory stats not available\")\n",
    "    else:\n",
    "        print(\"JAX memory_stats method not available\")\n",
    "    \n",
    "    # Use nvidia-smi command instead\n",
    "    try:\n",
    "        result = subprocess.run([\n",
    "            'nvidia-smi', \n",
    "            '--query-gpu=memory.used,memory.total', \n",
    "            '--format=csv,noheader,nounits'\n",
    "        ], capture_output=True, text=True, check=True)\n",
    "        \n",
    "        used, total = result.stdout.strip().split(', ')\n",
    "        used_gb = int(used) / 1024\n",
    "        total_gb = int(total) / 1024\n",
    "        print(f\"\\nNVIDIA GPU memory:\")\n",
    "        print(f\"  Used: {used_gb:.2f} GB\")\n",
    "        print(f\"  Total: {total_gb:.2f} GB\")\n",
    "        print(f\"  Utilization: {used_gb/total_gb*100:.1f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get GPU memory info: {e}\")\n",
    "\n",
    "# Run this before and during training\n",
    "check_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dc63a5",
   "metadata": {},
   "source": [
    "### Define dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "932f4c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bwilliams/mlx/week6/bb-finetune/ben_dev/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np  # or jax.numpy as jnp if needed\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "class TLDRDataset:\n",
    "    def __init__(self, data_dir, tokenizer, split, max_length=550):\n",
    "        \"\"\"\n",
    "        Load TLDR dataset from local parquet files.\n",
    "        \n",
    "        Args:\n",
    "            data_dir: Path to directory containing parquet files\n",
    "            tokenizer: Tokenizer to use\n",
    "            split: 'train', 'valid', or 'test'\n",
    "            max_length: Maximum sequence length\n",
    "        \"\"\"\n",
    "        # Load the parquet file\n",
    "        parquet_file = Path(data_dir) / f\"tldr_{split}.parquet\"\n",
    "        if not parquet_file.exists():\n",
    "            raise FileNotFoundError(f\"Dataset file not found: {parquet_file}\")\n",
    "        \n",
    "        df = pd.read_parquet(parquet_file)\n",
    "        \n",
    "        # Combine prompt and label for training (teacher forcing)\n",
    "        self.examples = [row[\"prompt\"] + row[\"label\"] for _, row in df.iterrows()]\n",
    "        \n",
    "        # Limit validation set size for faster iteration\n",
    "        if \"valid\" in split:\n",
    "            self.examples = self.examples[:2000]\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        print(f\"Loaded {len(self.examples)} examples from {parquet_file}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the text\n",
    "        enc = self.tokenizer(\n",
    "            self.examples[idx],\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": np.array(enc[\"input_ids\"], dtype=np.int32),\n",
    "            \"attention_mask\": np.array(enc[\"attention_mask\"], dtype=np.int32),\n",
    "            \"labels\": np.array(enc[\"input_ids\"], dtype=np.int32),  # teacher forcing\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43e3ab9",
   "metadata": {},
   "source": [
    "### Load model and tokeniser\n",
    "Stick to gpt2 now for compatibility, then move to qwen. GPT2 = 124m params, 500mb. qwen0.6b = 550m params, 2gb. So beware 3x qwen0.6b on my 8gb gpu might tank its memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22f82951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, FlaxAutoModelForCausalLM\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# 1. Tokenizer is identical\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Load the Flax (JAX) model\n",
    "#    .from_pretrained returns a FlaxAutoModelForCausalLM whose weights live in model.params\n",
    "model = FlaxAutoModelForCausalLM.from_pretrained(\"gpt2\", dtype=jnp.float16)\n",
    "\n",
    "# 3. If you‚Äôve added new tokens, resize just like in PyTorch:\n",
    "#    model = model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 4. Make sure padding is configured\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# 5. Pull out the parameter dict for training\n",
    "params = model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7792d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX device: cuda:0\n",
      "JAX memory stats not available\n",
      "\n",
      "NVIDIA GPU memory:\n",
      "  Used: 0.65 GB\n",
      "  Total: 8.00 GB\n",
      "  Utilization: 8.1%\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import subprocess\n",
    "\n",
    "def check_memory_usage():\n",
    "    # JAX device info\n",
    "    device = jax.devices()[0]\n",
    "    print(f\"JAX device: {device}\")\n",
    "    \n",
    "    # Try JAX memory stats (newer versions)\n",
    "    if hasattr(device, 'memory_stats'):\n",
    "        stats = device.memory_stats()\n",
    "        if stats is not None:\n",
    "            print(f\"JAX memory stats:\")\n",
    "            print(f\"  Bytes in use: {stats.get('bytes_in_use', 0) / 1024**3:.2f} GB\")\n",
    "            print(f\"  Pool bytes: {stats.get('pool_bytes', 0) / 1024**3:.2f} GB\")\n",
    "            print(f\"  Peak bytes: {stats.get('peak_bytes_in_use', 0) / 1024**3:.2f} GB\")\n",
    "        else:\n",
    "            print(\"JAX memory stats not available\")\n",
    "    else:\n",
    "        print(\"JAX memory_stats method not available\")\n",
    "    \n",
    "    # Use nvidia-smi command instead\n",
    "    try:\n",
    "        result = subprocess.run([\n",
    "            'nvidia-smi', \n",
    "            '--query-gpu=memory.used,memory.total', \n",
    "            '--format=csv,noheader,nounits'\n",
    "        ], capture_output=True, text=True, check=True)\n",
    "        \n",
    "        used, total = result.stdout.strip().split(', ')\n",
    "        used_gb = int(used) / 1024\n",
    "        total_gb = int(total) / 1024\n",
    "        print(f\"\\nNVIDIA GPU memory:\")\n",
    "        print(f\"  Used: {used_gb:.2f} GB\")\n",
    "        print(f\"  Total: {total_gb:.2f} GB\")\n",
    "        print(f\"  Utilization: {used_gb/total_gb*100:.1f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get GPU memory info: {e}\")\n",
    "\n",
    "# Run this before and during training\n",
    "check_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089f812e",
   "metadata": {},
   "source": [
    "### Inspect model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d4f9d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç JAX/Flax Model Inspection\n",
      "==================================================\n",
      "Model type: <class 'transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2LMHeadModel'>\n",
      "Model config: GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.53.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "Vocab size: 50257\n",
      "Hidden size: 768\n",
      "Number of layers: 12\n",
      "Number of attention heads: 12\n",
      "\n",
      "üìä Parameter Analysis\n",
      "==============================\n",
      "Total parameters: 124,439,808\n",
      "Total parameters (millions): 124.44M\n",
      "\n",
      "üèóÔ∏è Parameter Structure\n",
      "=========================\n",
      "transformer.h.0.attn.c_attn.bias: (2304,) (float32)\n",
      "transformer.h.0.attn.c_attn.kernel: (2304, 768) (float32)\n",
      "transformer.h.0.attn.c_proj.bias: (768,) (float32)\n",
      "transformer.h.0.attn.c_proj.kernel: (768, 768) (float32)\n",
      "transformer.h.0.ln_1.bias: (768,) (float32)\n",
      "transformer.h.0.ln_1.scale: (768,) (float32)\n",
      "transformer.h.0.ln_2.bias: (768,) (float32)\n",
      "transformer.h.0.ln_2.scale: (768,) (float32)\n",
      "transformer.h.0.mlp.c_fc.bias: (3072,) (float32)\n",
      "transformer.h.0.mlp.c_fc.kernel: (3072, 768) (float32)\n",
      "transformer.h.0.mlp.c_proj.bias: (768,) (float32)\n",
      "transformer.h.0.mlp.c_proj.kernel: (768, 3072) (float32)\n",
      "transformer.h.1.attn.c_attn.bias: (2304,) (float32)\n",
      "transformer.h.1.attn.c_attn.kernel: (2304, 768) (float32)\n",
      "transformer.h.1.attn.c_proj.bias: (768,) (float32)\n",
      "transformer.h.1.attn.c_proj.kernel: (768, 768) (float32)\n",
      "transformer.h.1.ln_1.bias: (768,) (float32)\n",
      "transformer.h.1.ln_1.scale: (768,) (float32)\n",
      "transformer.h.1.ln_2.bias: (768,) (float32)\n",
      "transformer.h.1.ln_2.scale: (768,) (float32)\n",
      "transformer.h.1.mlp.c_fc.bias: (3072,) (float32)\n",
      "transformer.h.1.mlp.c_fc.kernel: (3072, 768) (float32)\n",
      "transformer.h.1.mlp.c_proj.bias: (768,) (float32)\n",
      "transformer.h.1.mlp.c_proj.kernel: (768, 3072) (float32)\n",
      "transformer.h.10.attn.c_attn.bias: (2304,) (float32)\n",
      "transformer.h.10.attn.c_attn.kernel: (2304, 768) (float32)\n",
      "transformer.h.10.attn.c_proj.bias: (768,) (float32)\n",
      "transformer.h.10.attn.c_proj.kernel: (768, 768) (float32)\n",
      "transformer.h.10.ln_1.bias: (768,) (float32)\n",
      "transformer.h.10.ln_1.scale: (768,) (float32)\n",
      "transformer.h.10.ln_2.bias: (768,) (float32)\n",
      "transformer.h.10.ln_2.scale: (768,) (float32)\n",
      "transformer.h.10.mlp.c_fc.bias: (3072,) (float32)\n",
      "transformer.h.10.mlp.c_fc.kernel: (3072, 768) (float32)\n",
      "transformer.h.10.mlp.c_proj.bias: (768,) (float32)\n",
      "transformer.h.10.mlp.c_proj.kernel: (768, 3072) (float32)\n",
      "transformer.h.11.attn.c_attn.bias: (2304,) (float32)\n",
      "transformer.h.11.attn.c_attn.kernel: (2304, 768) (float32)\n",
      "transformer.h.11.attn.c_proj.bias: (768,) (float32)\n",
      "transformer.h.11.attn.c_proj.kernel: (768, 768) (float32)\n",
      "transformer.h.11.ln_1.bias: (768,) (float32)\n",
      "transformer.h.11.ln_1.scale: (768,) (float32)\n",
      "transformer.h.11.ln_2.bias: (768,) (float32)\n",
      "transformer.h.11.ln_2.scale: (768,) (float32)\n",
      "transformer.h.11.mlp.c_fc.bias: (3072,) (float32)\n",
      "transformer.h.11.mlp.c_fc.kernel: (3072, 768) (float32)\n",
      "transformer.h.11.mlp.c_proj.bias: (768,) (float32)\n",
      "transformer.h.11.mlp.c_proj.kernel: (768, 3072) (float32)\n",
      "transformer.h.2.attn.c_attn.bias: (2304,) (float32)\n",
      "transformer.h.2.attn.c_attn.kernel: (2304, 768) (float32)\n",
      "transformer.h.2.attn.c_proj.bias: (768,) (float32)\n",
      "transformer.h.2.attn.c_proj.kernel: (768, 768) (float32)\n",
      "transformer.h.2.ln_1.bias: (768,) (float32)\n",
      "transformer.h.2.ln_1.scale: (768,) (float32)\n",
      "transformer.h.2.ln_2.bias: (768,) (float32)\n",
      "transformer.h.2.ln_2.scale: (768,) (float32)\n",
      "transformer.h.2.mlp.c_fc.bias: (3072,) (float32)\n",
      "transformer.h.2.mlp.c_fc.kernel: (3072, 768) (float32)\n",
      "transformer.h.2.mlp.c_proj.bias: (768,) (float32)\n",
      "transformer.h.2.mlp.c_proj.kernel: (768, 3072) (float32)\n",
      "transformer.h.3.attn.c_attn.bias: (2304,) (float32)\n",
      "transformer.h.3.attn.c_attn.kernel: (2304, 768) (float32)\n",
      "transformer.h.3.attn.c_proj.bias: (768,) (float32)\n",
      "transformer.h.3.attn.c_proj.kernel: (768, 768) (float32)\n",
      "transformer.h.3.ln_1.bias: (768,) (float32)\n",
      "transformer.h.3.ln_1.scale: (768,) (float32)\n",
      "transformer.h.3.ln_2.bias: (768,) (float32)\n",
      "transformer.h.3.ln_2.scale: (768,) (float32)\n",
      "transformer.h.3.mlp.c_fc.bias: (3072,) (float32)\n",
      "transformer.h.3.mlp.c_fc.kernel: (3072, 768) (float32)\n",
      "transformer.h.3.mlp.c_proj.bias: (768,) (float32)\n",
      "transformer.h.3.mlp.c_proj.kernel: (768, 3072) (float32)\n",
      "transformer.h.4.attn.c_attn.bias: (2304,) (float32)\n",
      "transformer.h.4.attn.c_attn.kernel: (2304, 768) (float32)\n",
      "transformer.h.4.attn.c_proj.bias: (768,) (float32)\n",
      "transformer.h.4.attn.c_proj.kernel: (768, 768) (float32)\n",
      "transformer.h.4.ln_1.bias: (768,) (float32)\n",
      "transformer.h.4.ln_1.scale: (768,) (float32)\n",
      "transformer.h.4.ln_2.bias: (768,) (float32)\n",
      "transformer.h.4.ln_2.scale: (768,) (float32)\n",
      "transformer.h.4.mlp.c_fc.bias: (3072,) (float32)\n",
      "transformer.h.4.mlp.c_fc.kernel: (3072, 768) (float32)\n",
      "transformer.h.4.mlp.c_proj.bias: (768,) (float32)\n",
      "transformer.h.4.mlp.c_proj.kernel: (768, 3072) (float32)\n",
      "transformer.h.5.attn.c_attn.bias: (2304,) (float32)\n",
      "transformer.h.5.attn.c_attn.kernel: (2304, 768) (float32)\n",
      "transformer.h.5.attn.c_proj.bias: (768,) (float32)\n",
      "transformer.h.5.attn.c_proj.kernel: (768, 768) (float32)\n",
      "transformer.h.5.ln_1.bias: (768,) (float32)\n",
      "transformer.h.5.ln_1.scale: (768,) (float32)\n",
      "transformer.h.5.ln_2.bias: (768,) (float32)\n",
      "transformer.h.5.ln_2.scale: (768,) (float32)\n",
      "transformer.h.5.mlp.c_fc.bias: (3072,) (float32)\n",
      "transformer.h.5.mlp.c_fc.kernel: (3072, 768) (float32)\n",
      "transformer.h.5.mlp.c_proj.bias: (768,) (float32)\n",
      "transformer.h.5.mlp.c_proj.kernel: (768, 3072) (float32)\n",
      "transformer.h.6.attn.c_attn.bias: (2304,) (float32)\n",
      "transformer.h.6.attn.c_attn.kernel: (2304, 768) (float32)\n",
      "transformer.h.6.attn.c_proj.bias: (768,) (float32)\n",
      "transformer.h.6.attn.c_proj.kernel: (768, 768) (float32)\n",
      "transformer.h.6.ln_1.bias: (768,) (float32)\n",
      "transformer.h.6.ln_1.scale: (768,) (float32)\n",
      "transformer.h.6.ln_2.bias: (768,) (float32)\n",
      "transformer.h.6.ln_2.scale: (768,) (float32)\n",
      "transformer.h.6.mlp.c_fc.bias: (3072,) (float32)\n",
      "transformer.h.6.mlp.c_fc.kernel: (3072, 768) (float32)\n",
      "transformer.h.6.mlp.c_proj.bias: (768,) (float32)\n",
      "transformer.h.6.mlp.c_proj.kernel: (768, 3072) (float32)\n",
      "transformer.h.7.attn.c_attn.bias: (2304,) (float32)\n",
      "transformer.h.7.attn.c_attn.kernel: (2304, 768) (float32)\n",
      "transformer.h.7.attn.c_proj.bias: (768,) (float32)\n",
      "transformer.h.7.attn.c_proj.kernel: (768, 768) (float32)\n",
      "transformer.h.7.ln_1.bias: (768,) (float32)\n",
      "transformer.h.7.ln_1.scale: (768,) (float32)\n",
      "transformer.h.7.ln_2.bias: (768,) (float32)\n",
      "transformer.h.7.ln_2.scale: (768,) (float32)\n",
      "transformer.h.7.mlp.c_fc.bias: (3072,) (float32)\n",
      "transformer.h.7.mlp.c_fc.kernel: (3072, 768) (float32)\n",
      "transformer.h.7.mlp.c_proj.bias: (768,) (float32)\n",
      "transformer.h.7.mlp.c_proj.kernel: (768, 3072) (float32)\n",
      "transformer.h.8.attn.c_attn.bias: (2304,) (float32)\n",
      "transformer.h.8.attn.c_attn.kernel: (2304, 768) (float32)\n",
      "transformer.h.8.attn.c_proj.bias: (768,) (float32)\n",
      "transformer.h.8.attn.c_proj.kernel: (768, 768) (float32)\n",
      "transformer.h.8.ln_1.bias: (768,) (float32)\n",
      "transformer.h.8.ln_1.scale: (768,) (float32)\n",
      "transformer.h.8.ln_2.bias: (768,) (float32)\n",
      "transformer.h.8.ln_2.scale: (768,) (float32)\n",
      "transformer.h.8.mlp.c_fc.bias: (3072,) (float32)\n",
      "transformer.h.8.mlp.c_fc.kernel: (3072, 768) (float32)\n",
      "transformer.h.8.mlp.c_proj.bias: (768,) (float32)\n",
      "transformer.h.8.mlp.c_proj.kernel: (768, 3072) (float32)\n",
      "transformer.h.9.attn.c_attn.bias: (2304,) (float32)\n",
      "transformer.h.9.attn.c_attn.kernel: (2304, 768) (float32)\n",
      "transformer.h.9.attn.c_proj.bias: (768,) (float32)\n",
      "transformer.h.9.attn.c_proj.kernel: (768, 768) (float32)\n",
      "transformer.h.9.ln_1.bias: (768,) (float32)\n",
      "transformer.h.9.ln_1.scale: (768,) (float32)\n",
      "transformer.h.9.ln_2.bias: (768,) (float32)\n",
      "transformer.h.9.ln_2.scale: (768,) (float32)\n",
      "transformer.h.9.mlp.c_fc.bias: (3072,) (float32)\n",
      "transformer.h.9.mlp.c_fc.kernel: (3072, 768) (float32)\n",
      "transformer.h.9.mlp.c_proj.bias: (768,) (float32)\n",
      "transformer.h.9.mlp.c_proj.kernel: (768, 3072) (float32)\n",
      "transformer.ln_f.bias: (768,) (float32)\n",
      "transformer.ln_f.scale: (768,) (float32)\n",
      "transformer.wpe.embedding: (1024, 768) (float32)\n",
      "transformer.wte.embedding: (50257, 768) (float32)\n",
      "\n",
      "üíæ Estimated memory usage: 474.70 MB\n",
      "\n",
      "üî¢ Memory usage by dtype:\n",
      "  float32: 474.70 MB\n",
      "  float16: 237.35 MB\n",
      "  bfloat16: 237.35 MB\n"
     ]
    }
   ],
   "source": [
    "# Inspect JAX/Flax model architecture and parameters\n",
    "import jax\n",
    "from jax.tree_util import tree_map\n",
    "from flax.core import freeze\n",
    "\n",
    "print(\"üîç JAX/Flax Model Inspection\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Basic model info\n",
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Model config: {model.config}\")\n",
    "print(f\"Vocab size: {model.config.vocab_size}\")\n",
    "print(f\"Hidden size: {model.config.n_embd}\")\n",
    "print(f\"Number of layers: {model.config.n_layer}\")\n",
    "print(f\"Number of attention heads: {model.config.n_head}\")\n",
    "\n",
    "print(\"\\nüìä Parameter Analysis\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# 2. Count parameters (JAX way)\n",
    "def count_params(params):\n",
    "    \"\"\"Count total parameters in a JAX parameter tree\"\"\"\n",
    "    return sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
    "\n",
    "total_params = count_params(params)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Total parameters (millions): {total_params / 1_000_000:.2f}M\")\n",
    "\n",
    "# 3. Inspect parameter structure\n",
    "print(\"\\nüèóÔ∏è Parameter Structure\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "def print_param_shapes(params, prefix=\"\"):\n",
    "    \"\"\"Recursively print parameter shapes\"\"\"\n",
    "    if isinstance(params, dict):\n",
    "        for key, value in params.items():\n",
    "            print_param_shapes(value, f\"{prefix}.{key}\" if prefix else key)\n",
    "    else:\n",
    "        print(f\"{prefix}: {params.shape} ({params.dtype})\")\n",
    "\n",
    "print_param_shapes(params)\n",
    "\n",
    "# 4. Memory usage estimation\n",
    "def estimate_memory(params):\n",
    "    \"\"\"Estimate memory usage in MB\"\"\"\n",
    "    total_bytes = sum(x.nbytes for x in jax.tree_util.tree_leaves(params))\n",
    "    return total_bytes / (1024 ** 2)\n",
    "\n",
    "memory_mb = estimate_memory(params)\n",
    "print(f\"\\nüíæ Estimated memory usage: {memory_mb:.2f} MB\")\n",
    "\n",
    "# 5. Compare with different dtypes\n",
    "print(f\"\\nüî¢ Memory usage by dtype:\")\n",
    "print(f\"  float32: {memory_mb:.2f} MB\")\n",
    "print(f\"  float16: {memory_mb / 2:.2f} MB\") \n",
    "print(f\"  bfloat16: {memory_mb / 2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0299566c",
   "metadata": {},
   "source": [
    "### Do stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "758829f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 116722 examples from ../data/tldr_train.parquet\n",
      "Loaded 2000 examples from ../data/tldr_valid.parquet\n",
      "üìä Dataset loaded:\n",
      "  Training samples: 116722\n",
      "  Validation samples: 2000\n",
      "\n",
      "üìù Sample data shapes:\n",
      "  input_ids: (550,)\n",
      "  attention_mask: (550,)\n",
      "  labels: (550,)\n",
      "\n",
      "üìã Sample content:\n",
      "  First 100 chars of tokenized text: SUBREDDIT: r/relationships\n",
      "TITLE: I (f/22) have to figure out if I want to still know these girls or...\n",
      "  Input IDs (first 10): [   50 10526 22083 49828    25   374    14 39468  5748   198]\n",
      "  Labels (first 10): [   50 10526 22083 49828    25   374    14 39468  5748   198]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TLDRDataset(\"../data\", tokenizer, split=\"train\")\n",
    "val_dataset   = TLDRDataset(\"../data\", tokenizer, split=\"valid\")\n",
    "\n",
    "print(f\"üìä Dataset loaded:\")\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Preview a sample\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nüìù Sample data shapes:\")\n",
    "print(f\"  input_ids: {sample['input_ids'].shape}\")\n",
    "print(f\"  attention_mask: {sample['attention_mask'].shape}\")\n",
    "print(f\"  labels: {sample['labels'].shape}\")\n",
    "\n",
    "# Preview actual content\n",
    "print(f\"\\nüìã Sample content:\")\n",
    "print(f\"  First 100 chars of tokenized text: {train_dataset.examples[0][:100]}...\")\n",
    "print(f\"  Input IDs (first 10): {sample['input_ids'][:10]}\")\n",
    "print(f\"  Labels (first 10): {sample['labels'][:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cc33c5",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c36207a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating data loaders...\n",
      "üì¶ Testing batch loading...\n",
      "Batch shapes:\n",
      "  input_ids: (4, 550)\n",
      "  attention_mask: (4, 550)\n",
      "  labels: (4, 550)\n",
      "  Data type: int32\n",
      "\n",
      "üìä Batch counts:\n",
      "  Training batches: 29181\n",
      "  Validation batches: 500\n",
      "\n",
      "‚úÖ Data loader ready for JAX training!\n",
      "Batch shapes:\n",
      "  input_ids: (4, 550)\n",
      "  attention_mask: (4, 550)\n",
      "  labels: (4, 550)\n",
      "  Data type: int32\n",
      "\n",
      "üìä Batch counts:\n",
      "  Training batches: 29181\n",
      "  Validation batches: 500\n",
      "\n",
      "‚úÖ Data loader ready for JAX training!\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "def create_data_loader(dataset, batch_size, shuffle=True):\n",
    "    # Pre-tokenize everything into arrays\n",
    "    all_data = [dataset[i] for i in range(len(dataset))]\n",
    "    \n",
    "    # Convert to JAX arrays upfront\n",
    "    input_ids = jnp.array([x[\"input_ids\"] for x in all_data])\n",
    "    attention_mask = jnp.array([x[\"attention_mask\"] for x in all_data])\n",
    "    labels = jnp.array([x[\"labels\"] for x in all_data])\n",
    "    \n",
    "    num_batches = len(dataset) // batch_size  # Drop incomplete batches\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = random.permutation(random.PRNGKey(42), len(dataset))\n",
    "        input_ids = input_ids[indices]\n",
    "        attention_mask = attention_mask[indices]\n",
    "        labels = labels[indices]\n",
    "    \n",
    "    # Yield consistent-shaped batches\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        \n",
    "        yield {\n",
    "            \"input_ids\": input_ids[start_idx:end_idx],\n",
    "            \"attention_mask\": attention_mask[start_idx:end_idx], \n",
    "            \"labels\": labels[start_idx:end_idx]\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "print(\"üîÑ Creating data loaders...\")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 8  # Small batch for demo\n",
    "\n",
    "train_loader = create_data_loader(train_dataset, batch_size, shuffle=True)\n",
    "val_loader = create_data_loader(val_dataset, batch_size, shuffle=False)\n",
    "\n",
    "# Test the data loader\n",
    "print(\"üì¶ Testing batch loading...\")\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(f\"Batch shapes:\")\n",
    "print(f\"  input_ids: {batch['input_ids'].shape}\")\n",
    "print(f\"  attention_mask: {batch['attention_mask'].shape}\")\n",
    "print(f\"  labels: {batch['labels'].shape}\")\n",
    "print(f\"  Data type: {batch['input_ids'].dtype}\")\n",
    "\n",
    "# Show how many batches we'll have\n",
    "train_batches = (len(train_dataset) + batch_size - 1) // batch_size\n",
    "val_batches = (len(val_dataset) + batch_size - 1) // batch_size\n",
    "print(f\"\\nüìä Batch counts:\")\n",
    "print(f\"  Training batches: {train_batches}\")\n",
    "print(f\"  Validation batches: {val_batches}\")\n",
    "\n",
    "print(\"\\n‚úÖ Data loader ready for JAX training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98d3257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from flax.training import train_state\n",
    "\n",
    "# Initialize the optimizer\n",
    "tx = optax.adam(1e-5)\n",
    "\n",
    "# What is state? \n",
    "# It holds the model parameters, optimizer state, and any auxiliary data needed for training.\n",
    "state = train_state.TrainState.create(apply_fn=model.__call__,\n",
    "                                      params=params,\n",
    "                                      tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d75bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Starting Epoch 1/5 (29181 batches)\n",
      "Step 0 took: 37.69s\n",
      "Step 0 took: 37.69s\n",
      "Step 1 took: 0.24s\n",
      "Step 1 took: 0.24s\n",
      "Step 2 took: 0.24s\n",
      "Step 2 took: 0.24s\n",
      "Step 3 took: 0.24s\n",
      "Step 3 took: 0.24s\n",
      "Step 4 took: 0.24s\n",
      "Step 4 took: 0.24s\n",
      "Step 5 took: 0.25s\n",
      "Step 5 took: 0.25s\n",
      "  üìà Step 25/29181 (0.1%) - Avg loss (last 25): 4.0178\n",
      "  üìà Step 25/29181 (0.1%) - Avg loss (last 25): 4.0178\n",
      "  üìà Step 50/29181 (0.2%) - Avg loss (last 25): 4.3545\n",
      "  üìà Step 50/29181 (0.2%) - Avg loss (last 25): 4.3545\n",
      "  üìà Step 75/29181 (0.3%) - Avg loss (last 25): 4.3457\n",
      "  üìà Step 75/29181 (0.3%) - Avg loss (last 25): 4.3457\n",
      "  üìà Step 100/29181 (0.3%) - Avg loss (last 25): 3.9540\n",
      "  üìà Step 100/29181 (0.3%) - Avg loss (last 25): 3.9540\n",
      "  üìà Step 125/29181 (0.4%) - Avg loss (last 25): 3.9294\n",
      "  üìà Step 125/29181 (0.4%) - Avg loss (last 25): 3.9294\n",
      "  üìà Step 150/29181 (0.5%) - Avg loss (last 25): 3.9046\n",
      "  üìà Step 150/29181 (0.5%) - Avg loss (last 25): 3.9046\n",
      "  üìà Step 175/29181 (0.6%) - Avg loss (last 25): 3.8267\n",
      "  üìà Step 175/29181 (0.6%) - Avg loss (last 25): 3.8267\n",
      "  üìà Step 200/29181 (0.7%) - Avg loss (last 25): 3.7281\n",
      "  üìà Step 200/29181 (0.7%) - Avg loss (last 25): 3.7281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x78882fabb4c0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bwilliams/mlx/week6/bb-finetune/ben_dev/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üìà Step 225/29181 (0.8%) - Avg loss (last 25): 3.6598\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "from typing import Any, Dict, Tuple\n",
    "from tqdm import trange\n",
    "import time\n",
    "step_times = []\n",
    "\n",
    "\n",
    "# Define the training step function\n",
    "@jax.jit\n",
    "def train_step(\n",
    "    state: train_state.TrainState, \n",
    "    batch: Dict[str, jnp.ndarray],\n",
    "    dropout_rng: jax.random.PRNGKey\n",
    ") -> Tuple[train_state.TrainState, jnp.ndarray]:\n",
    "    \"\"\"\n",
    "    Perform a single training step (forward, loss, backward, update).\n",
    "    \n",
    "    Args:\n",
    "        state: TrainState containing params & optimizer state.\n",
    "        batch: Dict with keys \"input_ids\", \"attention_mask\", \"labels\" of shape (B, L).\n",
    "    \n",
    "    Returns:\n",
    "        new_state: Updated TrainState after applying gradients.\n",
    "        loss: Scalar loss for this batch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def loss_fn(params: Any) -> jnp.ndarray:\n",
    "        # Forward pass: get logits [batch, seq_len, vocab_size]\n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            params=params,\n",
    "            train=True,\n",
    "            dropout_rng=dropout_rng  # Use dropout RNG for training\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Causal LM shift: predict token t given inputs up to t-1\n",
    "        shift_logits = logits[..., :-1, :]           # drop last logit\n",
    "        shift_labels = batch[\"labels\"][..., 1:]       # drop first label\n",
    "        \n",
    "        # Compute per-token cross-entropy\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "            shift_logits, shift_labels\n",
    "        )  # shape: (batch, seq_len-1)\n",
    "        \n",
    "        # Mask out padding tokens from loss\n",
    "        mask = batch[\"attention_mask\"][..., 1:]       # same shift as labels\n",
    "        loss = jnp.sum(loss * mask) / jnp.sum(mask)   # mean over non-pad tokens\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    # Compute loss and gradients\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
    "    # Apply gradients to update parameters\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    return new_state, loss\n",
    "\n",
    "# Add memory monitoring and progress tracking to training\n",
    "def train_loop_with_monitoring(\n",
    "    state: train_state.TrainState,\n",
    "    train_loader: Any,\n",
    "    num_epochs: int = 3,\n",
    "    rng_key: jnp.ndarray = None\n",
    ") -> train_state.TrainState:\n",
    "    \"\"\"\n",
    "    Training loop with memory monitoring and progress tracking.\n",
    "    \"\"\"\n",
    "    if rng_key is None:\n",
    "        rng_key = random.PRNGKey(42)\n",
    "    \n",
    "    # Calculate total batches for progress tracking\n",
    "    train_batches = (len(train_dataset) + batch_size - 1) // batch_size\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        steps = 0\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        print(f\"üìä Starting Epoch {epoch}/{num_epochs} ({train_batches} batches)\")\n",
    "        \n",
    "        # Iterate over all batches\n",
    "        for batch in train_loader:\n",
    "            # Split RNG key for this step\n",
    "            rng_key, dropout_rng = random.split(rng_key)\n",
    "\n",
    "            # Single training step with timing\n",
    "            step_start = time.time()\n",
    "            state, loss = train_step(state, batch, dropout_rng)\n",
    "            step_time = time.time() - step_start\n",
    "            step_times.append(step_time)\n",
    "            \n",
    "            if steps <= 5:  # Print first 5 steps\n",
    "                print(f\"Step {steps} took: {step_time:.2f}s\")\n",
    "\n",
    "            # Process loss and increment counters\n",
    "            current_loss = float(loss)\n",
    "            epoch_loss += current_loss\n",
    "            running_loss += current_loss\n",
    "            steps += 1\n",
    "            \n",
    "            # Progress tracking every 50 steps\n",
    "            if steps % 25 == 0:\n",
    "                avg_loss_25 = running_loss / 25\n",
    "                progress_pct = (steps / train_batches) * 100\n",
    "                print(f\"  üìà Step {steps}/{train_batches} ({progress_pct:.1f}%) - Avg loss (last 25): {avg_loss_25:.4f}\")\n",
    "                running_loss = 0.0  # Reset running loss\n",
    "        \n",
    "        avg_loss = epoch_loss / steps\n",
    "        print(f\"‚úÖ Epoch {epoch} complete ‚Äî avg loss: {avg_loss:.4f}\")\n",
    "        print(\"üìä End of epoch memory:\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Use this instead of the regular train_loop\n",
    "num_epochs = 5  # Start with fewer epochs to see the pattern\n",
    "state = train_loop_with_monitoring(state, train_loader, num_epochs=num_epochs)\n",
    "\n",
    "# After training, `state.params` holds your fine-tuned model weights.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
