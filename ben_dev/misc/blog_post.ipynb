{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c36b7dbc",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- try lora\n",
    "- try reward model\n",
    "- ppo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2754682b",
   "metadata": {},
   "source": [
    "### 📊 Training Configuration - Fast Experimentation\n",
    "\n",
    "For quick experimentation, we'll use a subset of the dataset. Change `USE_FULL_DATASET` to `True` when you want to train on the complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cefa375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Configuration:\n",
      "  USE_FULL_DATASET: False\n",
      "  EXPERIMENT_SIZE: 200 samples\n",
      "  Estimated training time: ~0.2 minutes per epoch\n"
     ]
    }
   ],
   "source": [
    "# 🔧 EXPERIMENT CONFIGURATION\n",
    "USE_FULL_DATASET = True  # Set to True for full training, False for quick experiments\n",
    "EXPERIMENT_SIZE = 200    # Use only this many samples for quick testing\n",
    "\n",
    "print(f\"🎯 Configuration:\")\n",
    "print(f\"  USE_FULL_DATASET: {USE_FULL_DATASET}\")\n",
    "if not USE_FULL_DATASET:\n",
    "    print(f\"  EXPERIMENT_SIZE: {EXPERIMENT_SIZE} samples\")\n",
    "    print(f\"  Estimated training time: ~{EXPERIMENT_SIZE // 8 * 0.5 / 60:.1f} minutes per epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c9f0ac",
   "metadata": {},
   "source": [
    "### Sort mem\n",
    "Jax shotguns a bunch of memoery on the GPU but it doesn't need it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5820d38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\n",
    "os.environ['XLA_PYTHON_CLIENT_ALLOCATOR'] = 'platform'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5f60a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX device: cuda:0\n",
      "JAX memory stats not available\n",
      "\n",
      "NVIDIA GPU memory:\n",
      "  Used: 0.11 GB\n",
      "  Total: 8.00 GB\n",
      "  Utilization: 1.3%\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import subprocess\n",
    "\n",
    "def check_memory_usage():\n",
    "    # JAX device info\n",
    "    device = jax.devices()[0]\n",
    "    print(f\"JAX device: {device}\")\n",
    "    \n",
    "    # Try JAX memory stats (newer versions)\n",
    "    if hasattr(device, 'memory_stats'):\n",
    "        stats = device.memory_stats()\n",
    "        if stats is not None:\n",
    "            print(f\"JAX memory stats:\")\n",
    "            print(f\"  Bytes in use: {stats.get('bytes_in_use', 0) / 1024**3:.2f} GB\")\n",
    "            print(f\"  Pool bytes: {stats.get('pool_bytes', 0) / 1024**3:.2f} GB\")\n",
    "            print(f\"  Peak bytes: {stats.get('peak_bytes_in_use', 0) / 1024**3:.2f} GB\")\n",
    "        else:\n",
    "            print(\"JAX memory stats not available\")\n",
    "    else:\n",
    "        print(\"JAX memory_stats method not available\")\n",
    "    \n",
    "    # Use nvidia-smi command instead\n",
    "    try:\n",
    "        result = subprocess.run([\n",
    "            'nvidia-smi', \n",
    "            '--query-gpu=memory.used,memory.total', \n",
    "            '--format=csv,noheader,nounits'\n",
    "        ], capture_output=True, text=True, check=True)\n",
    "        \n",
    "        used, total = result.stdout.strip().split(', ')\n",
    "        used_gb = int(used) / 1024\n",
    "        total_gb = int(total) / 1024\n",
    "        print(f\"\\nNVIDIA GPU memory:\")\n",
    "        print(f\"  Used: {used_gb:.2f} GB\")\n",
    "        print(f\"  Total: {total_gb:.2f} GB\")\n",
    "        print(f\"  Utilization: {used_gb/total_gb*100:.1f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get GPU memory info: {e}\")\n",
    "\n",
    "# Run this before and during training\n",
    "check_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dc63a5",
   "metadata": {},
   "source": [
    "### Define dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "932f4c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bwilliams/mlx/week6/bb-finetune/ben_dev/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np  # or jax.numpy as jnp if needed\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "class TLDRDataset:\n",
    "    def __init__(self, data_dir, tokenizer, split, max_length=550):\n",
    "        \"\"\"\n",
    "        Load TLDR dataset from local parquet files.\n",
    "        \n",
    "        Args:\n",
    "            data_dir: Path to directory containing parquet files\n",
    "            tokenizer: Tokenizer to use\n",
    "            split: 'train', 'valid', or 'test'\n",
    "            max_length: Maximum sequence length\n",
    "        \"\"\"\n",
    "        # Load the parquet file\n",
    "        parquet_file = Path(data_dir) / f\"tldr_{split}.parquet\"\n",
    "        if not parquet_file.exists():\n",
    "            raise FileNotFoundError(f\"Dataset file not found: {parquet_file}\")\n",
    "        \n",
    "        df = pd.read_parquet(parquet_file)\n",
    "        \n",
    "        # Combine prompt and label for training (teacher forcing)\n",
    "        self.examples = [row[\"prompt\"] + row[\"label\"] for _, row in df.iterrows()]\n",
    "        \n",
    "        # Limit validation set size for faster iteration\n",
    "        if \"valid\" in split:\n",
    "            self.examples = self.examples[:2000]\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        print(f\"Loaded {len(self.examples)} examples from {parquet_file}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the text\n",
    "        enc = self.tokenizer(\n",
    "            self.examples[idx],\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": np.array(enc[\"input_ids\"], dtype=np.int32),\n",
    "            \"attention_mask\": np.array(enc[\"attention_mask\"], dtype=np.int32),\n",
    "            \"labels\": np.array(enc[\"input_ids\"], dtype=np.int32),  # teacher forcing\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43e3ab9",
   "metadata": {},
   "source": [
    "### Load model and tokeniser\n",
    "Stick to gpt2 now for compatibility, then move to qwen. GPT2 = 124m params, 500mb. qwen0.6b = 550m params, 2gb. So beware 3x qwen0.6b on my 8gb gpu might tank its memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22f82951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, FlaxAutoModelForCausalLM\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# 1. Tokenizer is identical\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Load the Flax (JAX) model\n",
    "#    .from_pretrained returns a FlaxAutoModelForCausalLM whose weights live in model.params\n",
    "model = FlaxAutoModelForCausalLM.from_pretrained(\"gpt2\", dtype=jnp.float16) # Back to your working config\n",
    "\n",
    "# 3. If you’ve added new tokens, resize just like in PyTorch:\n",
    "#    model = model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 4. Make sure padding is configured\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# 5. Pull out the parameter dict for training\n",
    "params = model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10bffdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX device: cuda:0\n",
      "JAX memory stats not available\n",
      "\n",
      "NVIDIA GPU memory:\n",
      "  Used: 0.65 GB\n",
      "  Total: 8.00 GB\n",
      "  Utilization: 8.1%\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import subprocess\n",
    "\n",
    "def check_memory_usage():\n",
    "    # JAX device info\n",
    "    device = jax.devices()[0]\n",
    "    print(f\"JAX device: {device}\")\n",
    "    \n",
    "    # Try JAX memory stats (newer versions)\n",
    "    if hasattr(device, 'memory_stats'):\n",
    "        stats = device.memory_stats()\n",
    "        if stats is not None:\n",
    "            print(f\"JAX memory stats:\")\n",
    "            print(f\"  Bytes in use: {stats.get('bytes_in_use', 0) / 1024**3:.2f} GB\")\n",
    "            print(f\"  Pool bytes: {stats.get('pool_bytes', 0) / 1024**3:.2f} GB\")\n",
    "            print(f\"  Peak bytes: {stats.get('peak_bytes_in_use', 0) / 1024**3:.2f} GB\")\n",
    "        else:\n",
    "            print(\"JAX memory stats not available\")\n",
    "    else:\n",
    "        print(\"JAX memory_stats method not available\")\n",
    "    \n",
    "    # Use nvidia-smi command instead\n",
    "    try:\n",
    "        result = subprocess.run([\n",
    "            'nvidia-smi', \n",
    "            '--query-gpu=memory.used,memory.total', \n",
    "            '--format=csv,noheader,nounits'\n",
    "        ], capture_output=True, text=True, check=True)\n",
    "        \n",
    "        used, total = result.stdout.strip().split(', ')\n",
    "        used_gb = int(used) / 1024\n",
    "        total_gb = int(total) / 1024\n",
    "        print(f\"\\nNVIDIA GPU memory:\")\n",
    "        print(f\"  Used: {used_gb:.2f} GB\")\n",
    "        print(f\"  Total: {total_gb:.2f} GB\")\n",
    "        print(f\"  Utilization: {used_gb/total_gb*100:.1f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get GPU memory info: {e}\")\n",
    "\n",
    "# Run this before and during training\n",
    "check_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089f812e",
   "metadata": {},
   "source": [
    "### Inspect model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d4f9d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 JAX/Flax Model Inspection\n",
      "==================================================\n",
      "Model type: <class 'transformers.models.gpt2.modeling_flax_gpt2.FlaxGPT2LMHeadModel'>\n",
      "Model config: GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.53.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "Vocab size: 50257\n",
      "Hidden size: 768\n",
      "Number of layers: 12\n",
      "Number of attention heads: 12\n",
      "\n",
      "📊 Parameter Analysis\n",
      "==============================\n",
      "Total parameters: 124,439,808\n",
      "Total parameters (millions): 124.44M\n",
      "\n",
      "🏗️ Parameter Structure\n",
      "=========================\n",
      "transformer.h.0.attn.c_attn.bias: (2304,) (float32)\n",
      "transformer.h.0.attn.c_attn.kernel: (2304, 768) (float32)\n",
      "transformer.h.0.attn.c_proj.bias: (768,) (float32)\n",
      "transformer.h.0.attn.c_proj.kernel: (768, 768) (float32)\n",
      "transformer.h.0.ln_1.bias: (768,) (float32)\n",
      "transformer.h.0.ln_1.scale: (768,) (float32)\n",
      "transformer.h.0.ln_2.bias: (768,) (float32)\n",
      "transformer.h.0.ln_2.scale: (768,) (float32)\n",
      "transformer.h.0.mlp.c_fc.bias: (3072,) (float32)\n",
      "transformer.h.0.mlp.c_fc.kernel: (3072, 768) (float32)\n",
      "transformer.h.0.mlp.c_proj.bias: (768,) (float32)\n",
      "transformer.h.0.mlp.c_proj.kernel: (768, 3072) (float32)\n",
      "transformer.h.1.attn.c_attn.bias: (2304,) (float32)\n",
      "transformer.h.1.attn.c_attn.kernel: (2304, 768) (float32)\n",
      "transformer.h.1.attn.c_proj.bias: (768,) (float32)\n",
      "transformer.h.1.attn.c_proj.kernel: (768, 768) (float32)\n",
      "transformer.h.1.ln_1.bias: (768,) (float32)\n",
      "transformer.h.1.ln_1.scale: (768,) (float32)\n",
      "transformer.h.1.ln_2.bias: (768,) (float32)\n",
      "transformer.h.1.ln_2.scale: (768,) (float32)\n",
      "transformer.h.1.mlp.c_fc.bias: (3072,) (float32)\n",
      "transformer.h.1.mlp.c_fc.kernel: (3072, 768) (float32)\n",
      "transformer.h.1.mlp.c_proj.bias: (768,) (float32)\n",
      "transformer.h.1.mlp.c_proj.kernel: (768, 3072) (float32)\n",
      "transformer.h.10.attn.c_attn.bias: (2304,) (float32)\n",
      "transformer.h.10.attn.c_attn.kernel: (2304, 768) (float32)\n",
      "transformer.h.10.attn.c_proj.bias: (768,) (float32)\n",
      "transformer.h.10.attn.c_proj.kernel: (768, 768) (float32)\n",
      "transformer.h.10.ln_1.bias: (768,) (float32)\n",
      "transformer.h.10.ln_1.scale: (768,) (float32)\n",
      "transformer.h.10.ln_2.bias: (768,) (float32)\n",
      "transformer.h.10.ln_2.scale: (768,) (float32)\n",
      "transformer.h.10.mlp.c_fc.bias: (3072,) (float32)\n",
      "transformer.h.10.mlp.c_fc.kernel: (3072, 768) (float32)\n",
      "transformer.h.10.mlp.c_proj.bias: (768,) (float32)\n",
      "transformer.h.10.mlp.c_proj.kernel: (768, 3072) (float32)\n",
      "transformer.h.11.attn.c_attn.bias: (2304,) (float32)\n",
      "transformer.h.11.attn.c_attn.kernel: (2304, 768) (float32)\n",
      "transformer.h.11.attn.c_proj.bias: (768,) (float32)\n",
      "transformer.h.11.attn.c_proj.kernel: (768, 768) (float32)\n",
      "transformer.h.11.ln_1.bias: (768,) (float32)\n",
      "transformer.h.11.ln_1.scale: (768,) (float32)\n",
      "transformer.h.11.ln_2.bias: (768,) (float32)\n",
      "transformer.h.11.ln_2.scale: (768,) (float32)\n",
      "transformer.h.11.mlp.c_fc.bias: (3072,) (float32)\n",
      "transformer.h.11.mlp.c_fc.kernel: (3072, 768) (float32)\n",
      "transformer.h.11.mlp.c_proj.bias: (768,) (float32)\n",
      "transformer.h.11.mlp.c_proj.kernel: (768, 3072) (float32)\n",
      "transformer.h.2.attn.c_attn.bias: (2304,) (float32)\n",
      "transformer.h.2.attn.c_attn.kernel: (2304, 768) (float32)\n",
      "transformer.h.2.attn.c_proj.bias: (768,) (float32)\n",
      "transformer.h.2.attn.c_proj.kernel: (768, 768) (float32)\n",
      "transformer.h.2.ln_1.bias: (768,) (float32)\n",
      "transformer.h.2.ln_1.scale: (768,) (float32)\n",
      "transformer.h.2.ln_2.bias: (768,) (float32)\n",
      "transformer.h.2.ln_2.scale: (768,) (float32)\n",
      "transformer.h.2.mlp.c_fc.bias: (3072,) (float32)\n",
      "transformer.h.2.mlp.c_fc.kernel: (3072, 768) (float32)\n",
      "transformer.h.2.mlp.c_proj.bias: (768,) (float32)\n",
      "transformer.h.2.mlp.c_proj.kernel: (768, 3072) (float32)\n",
      "transformer.h.3.attn.c_attn.bias: (2304,) (float32)\n",
      "transformer.h.3.attn.c_attn.kernel: (2304, 768) (float32)\n",
      "transformer.h.3.attn.c_proj.bias: (768,) (float32)\n",
      "transformer.h.3.attn.c_proj.kernel: (768, 768) (float32)\n",
      "transformer.h.3.ln_1.bias: (768,) (float32)\n",
      "transformer.h.3.ln_1.scale: (768,) (float32)\n",
      "transformer.h.3.ln_2.bias: (768,) (float32)\n",
      "transformer.h.3.ln_2.scale: (768,) (float32)\n",
      "transformer.h.3.mlp.c_fc.bias: (3072,) (float32)\n",
      "transformer.h.3.mlp.c_fc.kernel: (3072, 768) (float32)\n",
      "transformer.h.3.mlp.c_proj.bias: (768,) (float32)\n",
      "transformer.h.3.mlp.c_proj.kernel: (768, 3072) (float32)\n",
      "transformer.h.4.attn.c_attn.bias: (2304,) (float32)\n",
      "transformer.h.4.attn.c_attn.kernel: (2304, 768) (float32)\n",
      "transformer.h.4.attn.c_proj.bias: (768,) (float32)\n",
      "transformer.h.4.attn.c_proj.kernel: (768, 768) (float32)\n",
      "transformer.h.4.ln_1.bias: (768,) (float32)\n",
      "transformer.h.4.ln_1.scale: (768,) (float32)\n",
      "transformer.h.4.ln_2.bias: (768,) (float32)\n",
      "transformer.h.4.ln_2.scale: (768,) (float32)\n",
      "transformer.h.4.mlp.c_fc.bias: (3072,) (float32)\n",
      "transformer.h.4.mlp.c_fc.kernel: (3072, 768) (float32)\n",
      "transformer.h.4.mlp.c_proj.bias: (768,) (float32)\n",
      "transformer.h.4.mlp.c_proj.kernel: (768, 3072) (float32)\n",
      "transformer.h.5.attn.c_attn.bias: (2304,) (float32)\n",
      "transformer.h.5.attn.c_attn.kernel: (2304, 768) (float32)\n",
      "transformer.h.5.attn.c_proj.bias: (768,) (float32)\n",
      "transformer.h.5.attn.c_proj.kernel: (768, 768) (float32)\n",
      "transformer.h.5.ln_1.bias: (768,) (float32)\n",
      "transformer.h.5.ln_1.scale: (768,) (float32)\n",
      "transformer.h.5.ln_2.bias: (768,) (float32)\n",
      "transformer.h.5.ln_2.scale: (768,) (float32)\n",
      "transformer.h.5.mlp.c_fc.bias: (3072,) (float32)\n",
      "transformer.h.5.mlp.c_fc.kernel: (3072, 768) (float32)\n",
      "transformer.h.5.mlp.c_proj.bias: (768,) (float32)\n",
      "transformer.h.5.mlp.c_proj.kernel: (768, 3072) (float32)\n",
      "transformer.h.6.attn.c_attn.bias: (2304,) (float32)\n",
      "transformer.h.6.attn.c_attn.kernel: (2304, 768) (float32)\n",
      "transformer.h.6.attn.c_proj.bias: (768,) (float32)\n",
      "transformer.h.6.attn.c_proj.kernel: (768, 768) (float32)\n",
      "transformer.h.6.ln_1.bias: (768,) (float32)\n",
      "transformer.h.6.ln_1.scale: (768,) (float32)\n",
      "transformer.h.6.ln_2.bias: (768,) (float32)\n",
      "transformer.h.6.ln_2.scale: (768,) (float32)\n",
      "transformer.h.6.mlp.c_fc.bias: (3072,) (float32)\n",
      "transformer.h.6.mlp.c_fc.kernel: (3072, 768) (float32)\n",
      "transformer.h.6.mlp.c_proj.bias: (768,) (float32)\n",
      "transformer.h.6.mlp.c_proj.kernel: (768, 3072) (float32)\n",
      "transformer.h.7.attn.c_attn.bias: (2304,) (float32)\n",
      "transformer.h.7.attn.c_attn.kernel: (2304, 768) (float32)\n",
      "transformer.h.7.attn.c_proj.bias: (768,) (float32)\n",
      "transformer.h.7.attn.c_proj.kernel: (768, 768) (float32)\n",
      "transformer.h.7.ln_1.bias: (768,) (float32)\n",
      "transformer.h.7.ln_1.scale: (768,) (float32)\n",
      "transformer.h.7.ln_2.bias: (768,) (float32)\n",
      "transformer.h.7.ln_2.scale: (768,) (float32)\n",
      "transformer.h.7.mlp.c_fc.bias: (3072,) (float32)\n",
      "transformer.h.7.mlp.c_fc.kernel: (3072, 768) (float32)\n",
      "transformer.h.7.mlp.c_proj.bias: (768,) (float32)\n",
      "transformer.h.7.mlp.c_proj.kernel: (768, 3072) (float32)\n",
      "transformer.h.8.attn.c_attn.bias: (2304,) (float32)\n",
      "transformer.h.8.attn.c_attn.kernel: (2304, 768) (float32)\n",
      "transformer.h.8.attn.c_proj.bias: (768,) (float32)\n",
      "transformer.h.8.attn.c_proj.kernel: (768, 768) (float32)\n",
      "transformer.h.8.ln_1.bias: (768,) (float32)\n",
      "transformer.h.8.ln_1.scale: (768,) (float32)\n",
      "transformer.h.8.ln_2.bias: (768,) (float32)\n",
      "transformer.h.8.ln_2.scale: (768,) (float32)\n",
      "transformer.h.8.mlp.c_fc.bias: (3072,) (float32)\n",
      "transformer.h.8.mlp.c_fc.kernel: (3072, 768) (float32)\n",
      "transformer.h.8.mlp.c_proj.bias: (768,) (float32)\n",
      "transformer.h.8.mlp.c_proj.kernel: (768, 3072) (float32)\n",
      "transformer.h.9.attn.c_attn.bias: (2304,) (float32)\n",
      "transformer.h.9.attn.c_attn.kernel: (2304, 768) (float32)\n",
      "transformer.h.9.attn.c_proj.bias: (768,) (float32)\n",
      "transformer.h.9.attn.c_proj.kernel: (768, 768) (float32)\n",
      "transformer.h.9.ln_1.bias: (768,) (float32)\n",
      "transformer.h.9.ln_1.scale: (768,) (float32)\n",
      "transformer.h.9.ln_2.bias: (768,) (float32)\n",
      "transformer.h.9.ln_2.scale: (768,) (float32)\n",
      "transformer.h.9.mlp.c_fc.bias: (3072,) (float32)\n",
      "transformer.h.9.mlp.c_fc.kernel: (3072, 768) (float32)\n",
      "transformer.h.9.mlp.c_proj.bias: (768,) (float32)\n",
      "transformer.h.9.mlp.c_proj.kernel: (768, 3072) (float32)\n",
      "transformer.ln_f.bias: (768,) (float32)\n",
      "transformer.ln_f.scale: (768,) (float32)\n",
      "transformer.wpe.embedding: (1024, 768) (float32)\n",
      "transformer.wte.embedding: (50257, 768) (float32)\n",
      "\n",
      "💾 Estimated memory usage: 474.70 MB\n",
      "\n",
      "🔢 Memory usage by dtype:\n",
      "  float32: 474.70 MB\n",
      "  float16: 237.35 MB\n",
      "  bfloat16: 237.35 MB\n"
     ]
    }
   ],
   "source": [
    "# Inspect JAX/Flax model architecture and parameters\n",
    "import jax\n",
    "from jax.tree_util import tree_map\n",
    "from flax.core import freeze\n",
    "\n",
    "print(\"🔍 JAX/Flax Model Inspection\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Basic model info\n",
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Model config: {model.config}\")\n",
    "print(f\"Vocab size: {model.config.vocab_size}\")\n",
    "print(f\"Hidden size: {model.config.n_embd}\")\n",
    "print(f\"Number of layers: {model.config.n_layer}\")\n",
    "print(f\"Number of attention heads: {model.config.n_head}\")\n",
    "\n",
    "print(\"\\n📊 Parameter Analysis\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# 2. Count parameters (JAX way)\n",
    "def count_params(params):\n",
    "    \"\"\"Count total parameters in a JAX parameter tree\"\"\"\n",
    "    return sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
    "\n",
    "total_params = count_params(params)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Total parameters (millions): {total_params / 1_000_000:.2f}M\")\n",
    "\n",
    "# 3. Inspect parameter structure\n",
    "print(\"\\n🏗️ Parameter Structure\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "def print_param_shapes(params, prefix=\"\"):\n",
    "    \"\"\"Recursively print parameter shapes\"\"\"\n",
    "    if isinstance(params, dict):\n",
    "        for key, value in params.items():\n",
    "            print_param_shapes(value, f\"{prefix}.{key}\" if prefix else key)\n",
    "    else:\n",
    "        print(f\"{prefix}: {params.shape} ({params.dtype})\")\n",
    "\n",
    "print_param_shapes(params)\n",
    "\n",
    "# 4. Memory usage estimation\n",
    "def estimate_memory(params):\n",
    "    \"\"\"Estimate memory usage in MB\"\"\"\n",
    "    total_bytes = sum(x.nbytes for x in jax.tree_util.tree_leaves(params))\n",
    "    return total_bytes / (1024 ** 2)\n",
    "\n",
    "memory_mb = estimate_memory(params)\n",
    "print(f\"\\n💾 Estimated memory usage: {memory_mb:.2f} MB\")\n",
    "\n",
    "# 5. Compare with different dtypes\n",
    "print(f\"\\n🔢 Memory usage by dtype:\")\n",
    "print(f\"  float32: {memory_mb:.2f} MB\")\n",
    "print(f\"  float16: {memory_mb / 2:.2f} MB\") \n",
    "print(f\"  bfloat16: {memory_mb / 2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0299566c",
   "metadata": {},
   "source": [
    "### Do stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "758829f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 116722 examples from ../data/tldr_train.parquet\n",
      "Loaded 2000 examples from ../data/tldr_valid.parquet\n",
      "🧪 Using experiment mode: limiting to 200 samples\n",
      "📊 Dataset loaded:\n",
      "  Training samples: 200\n",
      "  Validation samples: 100\n",
      "  ⏱️  Estimated training time per epoch: ~0.2 minutes\n",
      "\n",
      "📝 Sample data shapes:\n",
      "  input_ids: (550,)\n",
      "  attention_mask: (550,)\n",
      "  labels: (550,)\n",
      "\n",
      "📋 Sample content:\n",
      "  First 100 chars of tokenized text: SUBREDDIT: r/relationships\n",
      "TITLE: I (f/22) have to figure out if I want to still know these girls or...\n",
      "  Input IDs (first 10): [   50 10526 22083 49828    25   374    14 39468  5748   198]\n",
      "  Labels (first 10): [   50 10526 22083 49828    25   374    14 39468  5748   198]\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = TLDRDataset(\"../data\", tokenizer, split=\"train\")\n",
    "val_dataset   = TLDRDataset(\"../data\", tokenizer, split=\"valid\")\n",
    "\n",
    "# Apply experiment configuration\n",
    "if not USE_FULL_DATASET:\n",
    "    print(f\"🧪 Using experiment mode: limiting to {EXPERIMENT_SIZE} samples\")\n",
    "    # Create a subset by limiting the examples list\n",
    "    train_dataset.examples = train_dataset.examples[:EXPERIMENT_SIZE]\n",
    "    # Keep validation dataset smaller too for quick validation\n",
    "    val_dataset.examples = val_dataset.examples[:min(100, len(val_dataset.examples))]\n",
    "\n",
    "print(f\"📊 Dataset loaded:\")\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "if not USE_FULL_DATASET:\n",
    "    estimated_time = (len(train_dataset) // 8) * 0.5 / 60  # batch_size=8, 0.5s per step\n",
    "    print(f\"  ⏱️  Estimated training time per epoch: ~{estimated_time:.1f} minutes\")\n",
    "\n",
    "# Preview a sample\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\n📝 Sample data shapes:\")\n",
    "print(f\"  input_ids: {sample['input_ids'].shape}\")\n",
    "print(f\"  attention_mask: {sample['attention_mask'].shape}\")\n",
    "print(f\"  labels: {sample['labels'].shape}\")\n",
    "\n",
    "# Preview actual content\n",
    "print(f\"\\n📋 Sample content:\")\n",
    "print(f\"  First 100 chars of tokenized text: {train_dataset.examples[0][:100]}...\")\n",
    "print(f\"  Input IDs (first 10): {sample['input_ids'][:10]}\")\n",
    "print(f\"  Labels (first 10): {sample['labels'][:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cc33c5",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c36207a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Creating data loaders...\n",
      "📦 Testing batch loading...\n",
      "Batch shapes:\n",
      "  input_ids: (8, 550)\n",
      "  attention_mask: (8, 550)\n",
      "  labels: (8, 550)\n",
      "  Data type: int32\n",
      "\n",
      "📊 Batch counts:\n",
      "  Training batches: 25\n",
      "  Validation batches: 13\n",
      "\n",
      "✅ Data loader ready for JAX training!\n",
      "Batch shapes:\n",
      "  input_ids: (8, 550)\n",
      "  attention_mask: (8, 550)\n",
      "  labels: (8, 550)\n",
      "  Data type: int32\n",
      "\n",
      "📊 Batch counts:\n",
      "  Training batches: 25\n",
      "  Validation batches: 13\n",
      "\n",
      "✅ Data loader ready for JAX training!\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "def create_data_loader(dataset, batch_size, shuffle=True):\n",
    "    # Pre-tokenize everything into arrays (your original working approach)\n",
    "    all_data = [dataset[i] for i in range(len(dataset))]\n",
    "    \n",
    "    # Convert to JAX arrays upfront\n",
    "    input_ids = jnp.array([x[\"input_ids\"] for x in all_data])\n",
    "    attention_mask = jnp.array([x[\"attention_mask\"] for x in all_data])\n",
    "    labels = jnp.array([x[\"labels\"] for x in all_data])\n",
    "    \n",
    "    num_batches = len(dataset) // batch_size  # Drop incomplete batches\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = random.permutation(random.PRNGKey(42), len(dataset))\n",
    "        input_ids = input_ids[indices]\n",
    "        attention_mask = attention_mask[indices]\n",
    "        labels = labels[indices]\n",
    "    \n",
    "    # Yield consistent-shaped batches\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        \n",
    "        yield {\n",
    "            \"input_ids\": input_ids[start_idx:end_idx],\n",
    "            \"attention_mask\": attention_mask[start_idx:end_idx], \n",
    "            \"labels\": labels[start_idx:end_idx]\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "print(\"🔄 Creating data loaders...\")\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 8  # Back to your working batch size\n",
    "\n",
    "train_loader = create_data_loader(train_dataset, BATCH_SIZE, shuffle=True)\n",
    "val_loader = create_data_loader(val_dataset, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Test the data loader\n",
    "print(\"📦 Testing batch loading...\")\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(f\"Batch shapes:\")\n",
    "print(f\"  input_ids: {batch['input_ids'].shape}\")\n",
    "print(f\"  attention_mask: {batch['attention_mask'].shape}\")\n",
    "print(f\"  labels: {batch['labels'].shape}\")\n",
    "print(f\"  Data type: {batch['input_ids'].dtype}\")\n",
    "\n",
    "# Show how many batches we'll have\n",
    "train_batches = (len(train_dataset) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "val_batches = (len(val_dataset) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "print(f\"\\n📊 Batch counts:\")\n",
    "print(f\"  Training batches: {train_batches}\")\n",
    "print(f\"  Validation batches: {val_batches}\")\n",
    "\n",
    "print(\"\\n✅ Data loader ready for JAX training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98d3257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from flax.training import train_state\n",
    "\n",
    "# Initialize the optimizer\n",
    "tx = optax.adam(1e-5)\n",
    "\n",
    "# What is state? \n",
    "# It holds the model parameters, optimizer state, and any auxiliary data needed for training.\n",
    "state = train_state.TrainState.create(apply_fn=model.__call__,\n",
    "                                      params=params,\n",
    "                                      tx=tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c070ba",
   "metadata": {},
   "source": [
    "### 🚀 Training Speed Summary\n",
    "\n",
    "**Understanding your training times:**\n",
    "\n",
    "- **Full dataset (116,722 samples)**: ~1.8 hours per epoch (14,590 batches × 0.45s each)\n",
    "- **Experiment mode (1,000 samples)**: ~1 minute per epoch (125 batches × 0.45s each)\n",
    "\n",
    "**First step is always slow (~40s)** due to JAX JIT compilation, then it runs at normal speed.\n",
    "\n",
    "**To switch between modes:**\n",
    "- `USE_FULL_DATASET = False` → Quick experiments (1,000 samples)\n",
    "- `USE_FULL_DATASET = True` → Full training (116,722 samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc4ca5d",
   "metadata": {},
   "source": [
    "### 💾 Model Checkpointing\n",
    "\n",
    "We'll use Flax's built-in checkpointing system for clean, versioned model saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb8de125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Tensorflow library not found, tensorflow.io.gfile operations will use native shim calls. GCS paths (i.e. 'gs://...') cannot be accessed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Checkpoints will be saved to: ../models\n",
      "🔄 Will keep 3 most recent checkpoints\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from flax.training import checkpoints\n",
    "import json\n",
    "\n",
    "# 📁 Checkpoint configuration\n",
    "CHECKPOINT_DIR = \"../models\"  # Correct relative path from ben_dev/misc/ to ben_dev/models/\n",
    "MAX_CHECKPOINTS = 3  # Keep only the 3 most recent checkpoints\n",
    "\n",
    "def save_checkpoint(state, epoch, checkpoint_dir=CHECKPOINT_DIR, max_to_keep=MAX_CHECKPOINTS):\n",
    "    \"\"\"\n",
    "    Save model checkpoint using Flax's built-in checkpointing.\n",
    "    \n",
    "    Args:\n",
    "        state: TrainState containing params, opt_state, and step\n",
    "        epoch: Current epoch number \n",
    "        checkpoint_dir: Directory to save checkpoints (can be relative)\n",
    "        max_to_keep: Maximum number of checkpoints to keep (auto-cleanup)\n",
    "    \"\"\"\n",
    "    # Convert relative path to absolute (Flax requires absolute paths)\n",
    "    checkpoint_dir = Path(checkpoint_dir).resolve()\n",
    "    \n",
    "    # Ensure checkpoint directory exists\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save the complete TrainState (params + optimizer state + step)\n",
    "    # Flax automatically handles versioning with step numbers\n",
    "    checkpoints.save_checkpoint(\n",
    "        ckpt_dir=str(checkpoint_dir),  # Convert Path to string for Flax\n",
    "        target=state,  # This saves the entire TrainState\n",
    "        step=state.step,  # Use training step for versioning\n",
    "        keep=max_to_keep,  # Auto-cleanup: keep only last N checkpoints\n",
    "        overwrite=True  # Allow overwriting if step already exists\n",
    "    )\n",
    "    \n",
    "    # Also save model config for easy loading later\n",
    "    config_path = checkpoint_dir / \"model_config.json\"\n",
    "    if not config_path.exists():\n",
    "        config_dict = {\n",
    "            \"model_type\": \"gpt2\",\n",
    "            \"vocab_size\": model.config.vocab_size,\n",
    "            \"n_embd\": model.config.n_embd,\n",
    "            \"n_layer\": model.config.n_layer,\n",
    "            \"n_head\": model.config.n_head,\n",
    "            \"max_length\": 550,  # Your max sequence length\n",
    "            \"dtype\": \"float16\"  # Your model dtype\n",
    "        }\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config_dict, f, indent=2)\n",
    "    \n",
    "    print(f\"💾 Checkpoint saved: epoch {epoch}, step {state.step}\")\n",
    "    print(f\"   📂 Location: {checkpoint_dir}/checkpoint_{state.step}\")\n",
    "\n",
    "\n",
    "print(f\"📂 Checkpoints will be saved to: {CHECKPOINT_DIR}\")\n",
    "print(f\"🔄 Will keep {MAX_CHECKPOINTS} most recent checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5d75bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Training:   0%|                                                                              | 0/125 [00:00<?, ?it/s]2025-07-17 18:38:31.330716: W external/xla/xla/hlo/transforms/simplifiers/hlo_rematerialization.cc:3021] Can't reduce memory use below 3.25GiB (3492387526 bytes) by rematerialization; only reduced to 3.78GiB (4061437030 bytes), down from 5.05GiB (5420351484 bytes) originally\n",
      "2025-07-17 18:38:31.330716: W external/xla/xla/hlo/transforms/simplifiers/hlo_rematerialization.cc:3021] Can't reduce memory use below 3.25GiB (3492387526 bytes) by rematerialization; only reduced to 3.78GiB (4061437030 bytes), down from 5.05GiB (5420351484 bytes) originally\n",
      "🚀 Training:   1%|▌                                                                   | 1/125 [00:41<1:26:08, 41.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️  Global step 1 took: 41.65s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Training:   2%|█                                                                     | 2/125 [00:42<36:43, 17.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️  Global step 2 took: 1.27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Training:   2%|█▋                                                                    | 3/125 [00:43<20:12,  9.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️  Global step 3 took: 0.44s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Training:   3%|██▏                                                                   | 4/125 [00:43<12:29,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️  Global step 4 took: 0.45s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Training:   4%|██▊                                                                   | 5/125 [00:44<08:14,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️  Global step 5 took: 0.44s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Training:   5%|███▎                                                                  | 6/125 [00:44<05:42,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️  Global step 6 took: 0.45s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Training:   6%|███▉                                                                  | 7/125 [00:45<04:06,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️  Global step 7 took: 0.45s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Training:   6%|████▍                                                                 | 8/125 [00:45<03:03,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️  Global step 8 took: 0.46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Training:   7%|█████                                                                 | 9/125 [00:46<02:21,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️  Global step 9 took: 0.46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Training:   8%|█████▌                                                               | 10/125 [00:46<01:53,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️  Global step 10 took: 0.46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Training:  19%|█████████████▏                                                       | 24/125 [00:53<00:47,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1 complete — avg loss: 3.5143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Training:  19%|█████████████▏                                                       | 24/125 [00:54<00:47,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint saved: epoch 1, step 24\n",
      "   📂 Location: /home/bwilliams/mlx/week6/bb-finetune/ben_dev/models/checkpoint_24\n",
      "📁 Checkpoint saved for epoch 1\n",
      "✅ Epoch 2 complete — avg loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Training:  19%|█████████████▏                                                       | 24/125 [00:56<00:47,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint saved: epoch 2, step 24\n",
      "   📂 Location: /home/bwilliams/mlx/week6/bb-finetune/ben_dev/models/checkpoint_24\n",
      "📁 Checkpoint saved for epoch 2\n",
      "✅ Epoch 3 complete — avg loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Training:  19%|█████████████▏                                                       | 24/125 [00:58<00:47,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint saved: epoch 3, step 24\n",
      "   📂 Location: /home/bwilliams/mlx/week6/bb-finetune/ben_dev/models/checkpoint_24\n",
      "📁 Checkpoint saved for epoch 3\n",
      "✅ Epoch 4 complete — avg loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Training:  19%|█████████████▏                                                       | 24/125 [01:00<00:47,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint saved: epoch 4, step 24\n",
      "   📂 Location: /home/bwilliams/mlx/week6/bb-finetune/ben_dev/models/checkpoint_24\n",
      "📁 Checkpoint saved for epoch 4\n",
      "✅ Epoch 5 complete — avg loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🚀 Training:  19%|█████████████▏                                                       | 24/125 [01:02<04:24,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Checkpoint saved: epoch 5, step 24\n",
      "   📂 Location: /home/bwilliams/mlx/week6/bb-finetune/ben_dev/models/checkpoint_24\n",
      "📁 Checkpoint saved for epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "from typing import Any, Dict, Tuple\n",
    "from tqdm import trange, tqdm\n",
    "import time\n",
    "step_times = []\n",
    "\n",
    "\n",
    "# Define the training step function\n",
    "@jax.jit\n",
    "def train_step(\n",
    "    state: train_state.TrainState, \n",
    "    batch: Dict[str, jnp.ndarray],\n",
    "    dropout_rng: jax.random.PRNGKey\n",
    ") -> Tuple[train_state.TrainState, jnp.ndarray]:\n",
    "    \"\"\"\n",
    "    Perform a single training step (forward, loss, backward, update).\n",
    "    \n",
    "    Args:\n",
    "        state: TrainState containing params & optimizer state.\n",
    "        batch: Dict with keys \"input_ids\", \"attention_mask\", \"labels\" of shape (B, L).\n",
    "    \n",
    "    Returns:\n",
    "        new_state: Updated TrainState after applying gradients.\n",
    "        loss: Scalar loss for this batch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def loss_fn(params: Any) -> jnp.ndarray:\n",
    "        # Forward pass: get logits [batch, seq_len, vocab_size]\n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            params=params,\n",
    "            train=True,\n",
    "            dropout_rng=dropout_rng  # Use dropout RNG for training\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Causal LM shift: predict token t given inputs up to t-1\n",
    "        shift_logits = logits[..., :-1, :]           # drop last logit\n",
    "        shift_labels = batch[\"labels\"][..., 1:]       # drop first label\n",
    "        \n",
    "        # Compute per-token cross-entropy\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "            shift_logits, shift_labels\n",
    "        )  # shape: (batch, seq_len-1)\n",
    "        \n",
    "        # Mask out padding tokens from loss\n",
    "        mask = batch[\"attention_mask\"][..., 1:]       # same shift as labels\n",
    "        loss = jnp.sum(loss * mask) / jnp.sum(mask)   # mean over non-pad tokens\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    # Compute loss and gradients\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
    "    # Apply gradients to update parameters\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    return new_state, loss\n",
    "\n",
    "# Add memory monitoring and progress tracking to training\n",
    "def train_loop(\n",
    "    state: train_state.TrainState,\n",
    "    train_loader: Any,\n",
    "    num_epochs: int = 3,\n",
    "    rng_key: jnp.ndarray = None\n",
    ") -> train_state.TrainState:\n",
    "    \"\"\"\n",
    "    Training loop with memory monitoring and progress tracking.\n",
    "    \"\"\"\n",
    "    if rng_key is None:\n",
    "        rng_key = random.PRNGKey(42)\n",
    "    \n",
    "    # Calculate total steps across all epochs\n",
    "    train_batches = (len(train_dataset) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    total_steps = train_batches * num_epochs\n",
    "\n",
    "    # Single progress bar for all training\n",
    "    pbar = tqdm(\n",
    "        total=total_steps,\n",
    "        desc=\"🚀 Training\",\n",
    "        ncols=120,\n",
    "        bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'\n",
    "    )\n",
    "    \n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            # Split RNG key for this step\n",
    "            rng_key, dropout_rng = random.split(rng_key)\n",
    "\n",
    "            # Single training step\n",
    "            step_start = time.time()\n",
    "            state, loss = train_step(state, batch, dropout_rng)\n",
    "            step_time = time.time() - step_start\n",
    "            \n",
    "            # Process loss and increment counters\n",
    "            current_loss = float(loss)\n",
    "            epoch_loss += current_loss\n",
    "            epoch_steps += 1\n",
    "            global_step += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'epoch': f'{epoch}/{num_epochs}',\n",
    "                'loss': f'{current_loss:.4f}',\n",
    "                'avg_loss': f'{epoch_loss/epoch_steps:.4f}',\n",
    "                'step_time': f'{step_time:.2f}s'\n",
    "            })\n",
    "            pbar.update(1)\n",
    "            \n",
    "            # Optional: Print timing for first few steps\n",
    "            if global_step <= 10:\n",
    "                tqdm.write(f\"⏱️  Global step {global_step} took: {step_time:.2f}s\")\n",
    "        \n",
    "        # Print epoch summary\n",
    "        avg_loss = epoch_loss / epoch_steps if epoch_steps > 0 else 0.0\n",
    "        tqdm.write(f\"✅ Epoch {epoch} complete — avg loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # 💾 Save checkpoint at end of each epoch\n",
    "        save_checkpoint(state, epoch)\n",
    "        tqdm.write(f\"📁 Checkpoint saved for epoch {epoch}\")\n",
    "    \n",
    "    pbar.close()\n",
    "    return state\n",
    "\n",
    "# Use this instead of the regular train_loop\n",
    "num_epochs = 5  # Start with fewer epochs to see the pattern\n",
    "state = train_loop(state, train_loader, num_epochs=num_epochs)\n",
    "\n",
    "# After training, `state.params` holds your fine-tuned model weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33370c5",
   "metadata": {},
   "source": [
    "### 🔄 Checkpoint Usage Examples\n",
    "\n",
    "Here's how to use the checkpoint system for different scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ac685bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_dir=CHECKPOINT_DIR, step=None):\n",
    "    \"\"\"\n",
    "    Load model checkpoint using Flax's built-in checkpointing.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_dir: Directory containing checkpoints (can be relative)\n",
    "        step: Specific step to load (None = latest)\n",
    "    \n",
    "    Returns:\n",
    "        restored_state: TrainState with loaded params/optimizer/step\n",
    "        epoch_info: Dict with loaded step information\n",
    "    \"\"\"\n",
    "    # Convert relative path to absolute (Flax requires absolute paths)\n",
    "    checkpoint_dir = Path(checkpoint_dir).resolve()\n",
    "    \n",
    "    if not checkpoint_dir.exists():\n",
    "        raise FileNotFoundError(f\"Checkpoint directory not found: {checkpoint_dir}\")\n",
    "    \n",
    "    # Create a dummy state for restoration template\n",
    "    # (Flax needs the structure to know what to restore)\n",
    "    dummy_state = train_state.TrainState.create(\n",
    "        apply_fn=model.__call__,\n",
    "        params=params,  # Use current params as template\n",
    "        tx=optax.adam(1e-5)  # Use current optimizer as template\n",
    "    )\n",
    "    \n",
    "    # Restore the checkpoint\n",
    "    restored_state = checkpoints.restore_checkpoint(\n",
    "        ckpt_dir=str(checkpoint_dir),  # Convert Path to string for Flax\n",
    "        target=dummy_state,  # Template for structure\n",
    "        step=step  # None = latest, or specify step number\n",
    "    )\n",
    "    \n",
    "    epoch_info = {\n",
    "        \"step\": restored_state.step,\n",
    "        \"approximate_epoch\": restored_state.step // ((len(train_dataset) // BATCH_SIZE))\n",
    "    }\n",
    "    \n",
    "    print(f\"📥 Checkpoint loaded: step {restored_state.step}\")\n",
    "    print(f\"   🔄 Approximate epoch: {epoch_info['approximate_epoch']}\")\n",
    "    \n",
    "    return restored_state, epoch_info\n",
    "\n",
    "# 🔍 Utility function to list available checkpoints\n",
    "def list_checkpoints(checkpoint_dir=CHECKPOINT_DIR):\n",
    "    \"\"\"List all available checkpoints in the directory.\"\"\"\n",
    "    # Convert relative path to absolute\n",
    "    checkpoint_dir = Path(checkpoint_dir).resolve()\n",
    "    \n",
    "    if not checkpoint_dir.exists():\n",
    "        print(f\"No checkpoint directory found: {checkpoint_dir}\")\n",
    "        return []\n",
    "    \n",
    "    # Get all checkpoint steps by scanning for checkpoint_* directories\n",
    "    checkpoint_pattern = checkpoint_dir / \"checkpoint_*\"\n",
    "    checkpoint_dirs = list(checkpoint_dir.glob(\"checkpoint_*\"))\n",
    "    \n",
    "    if not checkpoint_dirs:\n",
    "        print(f\"No checkpoints found in {checkpoint_dir}\")\n",
    "        return []\n",
    "    \n",
    "    # Extract step numbers from directory names\n",
    "    available_steps = []\n",
    "    for ckpt_dir in checkpoint_dirs:\n",
    "        try:\n",
    "            step = int(ckpt_dir.name.replace(\"checkpoint_\", \"\"))\n",
    "            available_steps.append(step)\n",
    "        except ValueError:\n",
    "            continue  # Skip invalid checkpoint directory names\n",
    "    \n",
    "    # Sort steps numerically\n",
    "    available_steps.sort()\n",
    "    \n",
    "    print(f\"📋 Available checkpoints in {checkpoint_dir}:\")\n",
    "    for step in available_steps:\n",
    "        approximate_epoch = step // ((len(train_dataset) // BATCH_SIZE))\n",
    "        print(f\"   Step {step} (≈ epoch {approximate_epoch})\")\n",
    "    \n",
    "    return available_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42a0a0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "🔍 LISTING AVAILABLE CHECKPOINTS\n",
      "==================================================\n",
      "📋 Available checkpoints in /home/bwilliams/mlx/week6/bb-finetune/ben_dev/models:\n",
      "   Step 24 (≈ epoch 0)\n",
      "\n",
      "==================================================\n",
      "📥 LOADING LATEST CHECKPOINT\n",
      "==================================================\n",
      "💡 Uncomment the lines above once you have saved checkpoints!\n",
      "\n",
      "==================================================\n",
      "📥 LOADING SPECIFIC CHECKPOINT\n",
      "==================================================\n",
      "💡 Use load_checkpoint(step=N) to load a specific checkpoint\n",
      "\n",
      "==================================================\n",
      "🚀 RESUMING TRAINING\n",
      "==================================================\n",
      "\n",
      "# To resume training from the latest checkpoint:\n",
      "restored_state, info = load_checkpoint()\n",
      "state = restored_state\n",
      "\n",
      "# Then continue training normally:\n",
      "state = train_loop(state, train_loader, num_epochs=additional_epochs)\n",
      "\n",
      "# The training will pick up from where it left off!\n",
      "\n",
      "✅ Checkpoint examples ready to use!\n"
     ]
    }
   ],
   "source": [
    "# 📋 Example 1: List all available checkpoints\n",
    "print(\"=\" * 50)\n",
    "print(\"🔍 LISTING AVAILABLE CHECKPOINTS\")\n",
    "print(\"=\" * 50)\n",
    "available_steps = list_checkpoints()\n",
    "\n",
    "# 📥 Example 2: Load the latest checkpoint (for resuming training)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"📥 LOADING LATEST CHECKPOINT\")\n",
    "print(\"=\" * 50)\n",
    "try:\n",
    "    # Uncomment these lines when you have checkpoints saved:\n",
    "    # restored_state, info = load_checkpoint()\n",
    "    # print(f\"Resumed from step {info['step']}, approximate epoch {info['approximate_epoch']}\")\n",
    "    # state = restored_state  # Use this state to continue training\n",
    "    print(\"💡 Uncomment the lines above once you have saved checkpoints!\")\n",
    "except Exception as e:\n",
    "    print(f\"No checkpoints found yet: {e}\")\n",
    "\n",
    "# 📥 Example 3: Load a specific checkpoint by step number\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"📥 LOADING SPECIFIC CHECKPOINT\")\n",
    "print(\"=\" * 50)\n",
    "try:\n",
    "    # Example: load checkpoint from step 500\n",
    "    # restored_state, info = load_checkpoint(step=500)\n",
    "    print(\"💡 Use load_checkpoint(step=N) to load a specific checkpoint\")\n",
    "except Exception as e:\n",
    "    print(f\"Specific checkpoint example: {e}\")\n",
    "\n",
    "# 🚀 Example 4: Resume training from checkpoint\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"🚀 RESUMING TRAINING\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\"\"\n",
    "# To resume training from the latest checkpoint:\n",
    "restored_state, info = load_checkpoint()\n",
    "state = restored_state\n",
    "\n",
    "# Then continue training normally:\n",
    "state = train_loop(state, train_loader, num_epochs=additional_epochs)\n",
    "\n",
    "# The training will pick up from where it left off!\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Checkpoint examples ready to use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8008c483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
